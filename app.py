import streamlit as st
import google.generativeai as genai
import json
import os
import datetime
import time
import zipfile
import io
import re
from duckduckgo_search import DDGS

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ---
st.set_page_config(
    page_title="THE COUNCIL V15 | Smart Selection",
    page_icon="ğŸ‘ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- Ù…Ù„Ù Ø§Ù„Ø°Ø§ÙƒØ±Ø© ---
MEMORY_FILE = "council_history.json"

# --- Ø¯ÙˆØ§Ù„ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© ---
def load_memory():
    if os.path.exists(MEMORY_FILE):
        try:
            with open(MEMORY_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return []
    return []

def save_memory(record):
    history = load_memory()
    history.append(record)
    with open(MEMORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=4)

def clear_memory():
    if os.path.exists(MEMORY_FILE):
        os.remove(MEMORY_FILE)

def get_relevant_context(query):
    history = load_memory()
    if not history: return ""
    recent = history[-3:]
    context_text = ""
    for item in recent:
        context_text += f"- {item['date']}: {item.get('summary', 'N/A')}\n"
    return context_text

# --- Ø¯Ø§Ù„Ø© Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª ---
def search_web(query):
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(query, max_results=3))
            if results:
                return "\n".join([f"- {r['title']}: {r['body']}" for r in results])
            return "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬."
    except Exception as e:
        return f"Ø®Ø·Ø£ Ø¥Ù†ØªØ±Ù†Øª: {str(e)}"

# --- Ø¯Ø§Ù„Ø© Ø¶ØºØ· Ø§Ù„Ù…Ù„ÙØ§Øª ---
def create_zip_from_response(text):
    zip_buffer = io.BytesIO()
    code_blocks = re.findall(r"```(\w+)?\n(.*?)```", text, re.DOTALL)
    if not code_blocks: return None
    with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zip_file:
        for i, (lang, code) in enumerate(code_blocks):
            lang = lang.lower().strip() if lang else "txt"
            ext = "txt"
            if "python" in lang or "py" in lang: ext = "py"
            elif "html" in lang: ext = "html"
            elif "css" in lang: ext = "css"
            elif "javascript" in lang or "js" in lang: ext = "js"
            elif "json" in lang: ext = "json"
            filename_match = re.search(r"filename:\s*([\w\-\.]+)", code)
            filename = filename_match.group(1) if filename_match else f"file_{i+1}.{ext}"
            zip_file.writestr(filename, code)
    zip_buffer.seek(0)
    return zip_buffer

# --- Ø¯Ø§Ù„Ø© Ø¬Ù„Ø¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ø°ÙƒÙŠØ© (Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¬Ø¯ÙŠØ¯) ---
def get_clean_model_list():
    """
    ØªØ¹ÙŠØ¯ Ù‚Ø§Ø¦Ù…Ø© Ù†Ø¸ÙŠÙØ© Ø¨Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙØ¹Ù„ÙŠØ§Ù‹ ÙÙ‚Ø·ØŒ
    Ù…Ø¹ Ø¥Ø¹Ø·Ø§Ø¡ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø±Ø© (Flash/Pro).
    """
    # Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© (Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„ØªÙŠ Ù†Ø±ÙŠØ¯Ù‡Ø§ Ø¯Ø§Ø¦Ù…Ø§Ù‹ ÙÙŠ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©)
    priority_models = ["gemini-1.5-flash", "gemini-1.5-pro", "gemini-pro"]
    
    try:
        fetched_models = []
        for m in genai.list_models():
            # Ù†Ø£Ø®Ø° ÙÙ‚Ø· Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„ØªÙŠ ØªÙˆÙ„Ø¯ Ù†ØµÙˆØµØ§Ù‹
            if 'generateContent' in m.supported_generation_methods:
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³Ù… (Ø­Ø°Ù models/)
                clean_name = m.name.replace("models/", "")
                fetched_models.append(clean_name)
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù…: Ø§Ø¨Ø¯Ø£ Ø¨Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©ØŒ Ø«Ù… Ø£Ø¶Ù Ø§Ù„Ø¨Ø§Ù‚ÙŠ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙƒØ±Ø±Ø§Ù‹
        final_list = priority_models.copy()
        for m in fetched_models:
            if m not in final_list:
                final_list.append(m)
                
        return final_list
    except:
        # ÙÙŠ Ø­Ø§Ù„ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ØŒ Ù†Ø¹ÙˆØ¯ Ù„Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙŠØ¯ÙˆÙŠØ© Ø§Ù„Ø¢Ù…Ù†Ø©
        return priority_models

# --- Ø§Ù„ØªØµÙ…ÙŠÙ… ---
st.markdown("""
<style>
    .stApp { background-color: #050505; color: #e0e0e0; }
    h1, h2, h3 { font-family: 'Georgia', serif; color: #d4af37; }
    .advisor-card { background-color: #111; padding: 15px; border-radius: 8px; border-left: 4px solid #444; margin-bottom: 15px; }
    .devil-card { background-color: #1a0505; padding: 15px; border-radius: 8px; border-left: 4px solid #ff0000; color: #ffcccc; }
    .overlord-card { background-color: #000; padding: 25px; border: 2px solid #d4af37; border-radius: 12px; }
</style>
""", unsafe_allow_html=True)

# --- Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ---
try:
    API_KEY = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=API_KEY)
except:
    st.error("âš ï¸ Ù…ÙØªØ§Ø­ API Ù…ÙÙ‚ÙˆØ¯.")
    st.stop()

# --- Sidebar ---
with st.sidebar:
    st.header("âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
    
    # --- Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø·ÙˆØ±Ø© ---
    clean_models = get_clean_model_list()
    selected_model_name = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ù…Ø­Ø±Ùƒ (Ø§Ù„Ø£Ø³Ø±Ø¹ Ø£ÙˆÙ„Ø§Ù‹):", clean_models, index=0)
    
    # Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ø¶Ø§ÙØ© Ø¨Ø§Ø¯Ø¦Ø© models/ Ù„Ø£Ù† API ÙŠØ­ØªØ§Ø¬Ù‡Ø§ Ø¨Ø±Ù…Ø¬ÙŠØ§Ù‹
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø§Ø³Ù… Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„ÙŠÙ‡Ø§ Ø£ØµÙ„Ø§Ù‹
    final_model_id = selected_model_name if selected_model_name.startswith("models/") else f"models/{selected_model_name}"
    
    st.caption(f"Ø§Ù„Ù…Ø¹Ø±Ù Ø§Ù„ØªÙ‚Ù†ÙŠ: `{final_model_id}`")
    
    st.divider()
    enable_internet = st.checkbox("Ø¨Ø­Ø« Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª (Web)", value=True)
    enable_memory = st.checkbox("Ø§Ù„Ø°Ø§ÙƒØ±Ø© (History)", value=True)
    
    st.divider()
    if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"):
        clear_memory()
        st.rerun()

# --- AI Engine ---
def ask_gemini(prompt, sys_instruction, model_id):
    try:
        model = genai.GenerativeModel(model_id)
        full_payload = f"System Role: {sys_instruction}\n\nTask: {prompt}"
        response = model.generate_content(full_payload)
        return response.text
    except Exception as e:
        if "429" in str(e): return "âš ï¸ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù…Ø´ØºÙˆÙ„ (ØªØ¬Ø§ÙˆØ²Øª Ø§Ù„Ø­Ø¯). Ø¬Ø±Ø¨ Flash."
        if "404" in str(e): return "âš ï¸ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ… ÙÙŠ Ù…Ù†Ø·Ù‚ØªÙƒØŒ Ø§Ø®ØªØ± ØºÙŠØ±Ù‡."
        return f"Error: {str(e)}"

# --- Main UI ---
st.markdown("<h1 style='text-align: center;'>ğŸ›ï¸ THE COUNCIL V15</h1>", unsafe_allow_html=True)

problem = st.text_area("Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ:", height=100)

advisors = {
    "Ø§Ù„Ù…Ø®Ø·Ø· ğŸ“": { "sys": "Ø£Ù†Øª Ø§Ù„Ù…Ø®Ø·Ø·. Ø¶Ø¹ Ø®Ø·Ø© Ø¹Ù…Ù„ Ø¯Ù‚ÙŠÙ‚Ø©." },
    "Ø§Ù„Ø´ÙŠØ·Ø§Ù†ÙŠ ğŸ˜ˆ": { "sys": "Ø£Ù†Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø´ÙŠØ·Ø§Ù†ÙŠ. Ø§Ø³ØªØºÙ„ ÙƒÙ„ Ø«ØºØ±Ø©ØŒ Ø§Ø³Ø­Ù‚ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†ØŒ ÙÙƒØ± Ø¨Ø§Ù„Ø±Ø¨Ø­ ÙÙ‚Ø·." },
    "Ø§Ù„Ù…Ø¨Ø±Ù…Ø¬ ğŸ’»": { "sys": "Ø£Ù†Øª Ø§Ù„Ù…Ø¨Ø±Ù…Ø¬. Ø§ÙƒØªØ¨ Ø§Ù„ÙƒÙˆØ¯. Ø¶Ø¹ ØªØ¹Ù„ÙŠÙ‚ # filename: name.ext Ø£ÙˆÙ„Ø§Ù‹." },
    "Ø§Ù„Ø£Ù…Ù† ğŸ›¡ï¸": { "sys": "Ø£Ù†Øª Ø§Ù„Ø£Ù…Ù†. Ø§ÙØ­Øµ Ø§Ù„Ø®Ø·Ø©." }
}

if st.button("ØªÙ†ÙÙŠØ° âš¡", use_container_width=True):
    if not problem:
        st.warning("Ø£Ø¯Ø®Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
    else:
        results = st.container()
        progress_bar = st.progress(0)
        status = st.empty()
        
        # 1. Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        data_packet = f"Ø§Ù„Ù…Ù‡Ù…Ø©: {problem}\n\n"
        
        if enable_internet:
            status.text("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ©...")
            web_res = search_web(problem)
            data_packet += f"--- Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª ---\n{web_res}\n\n"
            
        if enable_memory:
            status.text("Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø£Ø±Ø´ÙŠÙ...")
            mem_res = get_relevant_context(problem)
            data_packet += f"--- Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© ---\n{mem_res}\n\n"
        
        # 2. Ø§Ù„Ù…Ø³ØªØ´Ø§Ø±ÙŠÙ†
        with results:
            cols = st.columns(2)
            total = len(advisors) + 1
            curr = 0
            full_report = data_packet
            
            for idx, (name, info) in enumerate(advisors.items()):
                status.text(f"Ø§Ø³ØªØ´Ø§Ø±Ø© {name}...")
                with cols[idx % 2]:
                    time.sleep(1) # Ù…Ù†Ø¹ Ø§Ù„Ø­Ø¸Ø±
                    reply = ask_gemini(data_packet, info["sys"], final_model_id)
                    full_report += f"--- {name} ---\n{reply}\n\n"
                    st.markdown(f"<div class='advisor-card'><b>{name}</b><br>{reply}</div>", unsafe_allow_html=True)
                curr += 1
                progress_bar.progress(curr / total)
            
            # 3. Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹
            st.markdown("---")
            status.text("Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø£Ø¹Ø¸Ù… ÙŠØªØ®Ø° Ø§Ù„Ù‚Ø±Ø§Ø±...")
            overlord_sys = "Ø£Ù†Øª Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø£Ø¹Ø¸Ù…. Ø§Ø¯Ù…Ø¬ Ø§Ù„Ø¢Ø±Ø§Ø¡ ÙˆØ§ÙƒØªØ¨ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¨ØªØ¹Ù„ÙŠÙ‚Ø§Øª filename."
            final = ask_gemini(full_report, overlord_sys, final_model_id)
            
            st.markdown(f"<div class='overlord-card'>{final}</div>", unsafe_allow_html=True)
            
            # 4. Ø§Ù„Ù…Ù„ÙØ§Øª
            zip_data = create_zip_from_response(final)
            if zip_data:
                st.download_button("ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹", zip_data, "project.zip", "application/zip")
            
            # 5. Ø§Ù„Ø­ÙØ¸
            save_memory({"date": str(datetime.datetime.now()), "summary": final[:100] + "..."})
            progress_bar.progress(1.0)
            status.text("âœ… ØªÙ….")
